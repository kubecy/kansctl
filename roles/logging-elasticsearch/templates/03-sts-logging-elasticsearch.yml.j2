---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logging-elasticsearch
  namespace: logging
  labels:
    chart: "logging-elasticsearch"
    app: "logging-elasticsearch"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: logging-elasticsearch-headless
  selector:
    matchLabels:
      app: "logging-elasticsearch"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi
      storageClassName: logging-elasticsearch-storage
  template:
    metadata:
      name: "logging-elasticsearch"
      labels:
        chart: "logging-elasticsearch"
        app: "logging-elasticsearch"
      annotations:
        prometheus.io/scrape: "true"  # 添加注解，允许Prometheus自动发现
        prometheus.io/port: "9114"    # Exporter暴露的端口
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "logging-elasticsearch"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "{{ service.elasticsearch }}"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
      - name: fix-data-permissions  # 新增权限修复容器
        image: "{{ service.elasticsearch }}" 
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          runAsUser: 0  # 需要 root 权限
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data

      - name: increase-fd-ulimit
        image: "{{ service.elasticsearch }}" 
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          runAsUser: 0

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "{{ service.elasticsearch }}" 
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - bash
              - -c
              - |
                set -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "elastic:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "logging-elasticsearch-0,logging-elasticsearch-1,logging-elasticsearch-2"
          - name: discovery.seed_hosts
            value: "logging-elasticsearch-headless.logging.svc.cluster.local"
          - name: cluster.name
            value: "logging-elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: cluster.deprecation_indexing.enabled
            value: "false"
          - name: discovery.initial_state_timeout
            value: "5m"  # 增加初始化超时时间
          - name: discovery.zen.minimum_master_nodes
            value: "2"
          - name: discovery.zen.ping.unicast.hosts.resolve_timeout
            value: "30s"
          - name: node.roles
            value: "master,data,ingest"
          ## 已弃用
          #- name: node.data
          #  value: "true"
          #- name: node.ingest
          #  value: "true"
          #- name: node.master
          #  value: "true"
          #- name: node.ml
          #  value: "true"
          #- name: node.remote_cluster_client
          #  value: "true"
        volumeMounts:
          - name: "data"
            mountPath: /usr/share/elasticsearch/data

      # ============== 添加Elasticsearch Exporter容器 ==============
      - name: elastic-exporter
        image: registry.cn-guangzhou.aliyuncs.com/kansctl/elasticsearch-exporter:v1.9.0 
        #command:
          #- /bin/elasticsearch_exporter
        args:
          - --es.uri=http://localhost:9200  # 修改为localhost访问同Pod的ES
          - --es.all
        securityContext:
          capabilities:
            drop:
              - SETPCAP
              - MKNOD
              - AUDIT_WRITE
              - CHOWN
              - NET_RAW
              - DAC_OVERRIDE
              - FOWNER
              - FSETID
              - KILL
              - SETGID
              - SETUID
              - NET_BIND_SERVICE
              - SYS_CHROOT
              - SETFCAP
          runAsNonRoot: true
          runAsUser: 1000
          readOnlyRootFilesystem: true
        ports:
          - containerPort: 9114
            name: metrics
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9114
          initialDelaySeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9114
          initialDelaySeconds: 10
          timeoutSeconds: 10
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 25m
            memory: 64Mi
